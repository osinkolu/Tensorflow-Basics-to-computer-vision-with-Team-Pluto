{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a neural network using Tensorflow/Keras\n",
    "\n",
    "### Author : Olufemi Victor Tolulope\n",
    "\n",
    "###### Team: Project team Pluto: Basics to Computer Vision with Tensorflow.\n",
    "\n",
    "#### Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the course of the 2nd week with team pluto, we learned how to build a simple neural network using Keras API (of course with Tensorflow as the backend.). In my work here, i used the simple neural network (The feed forward neural network to be precise) to classify images where masks are worn vs images where no masks are worn. Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the necessary packages from keras, i also import pandas here to read the csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the csv file as datafame using pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>tfymlmdkpzkqdjhdxyhnoeuqszxphw.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>rxgismgsvmaayzjarbfjaljhqmpbrt.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>uuzshfrhkgrkolhwdvliqauzulurnz.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>mjspxsagzusaznvnyxgamtrlqkqklp.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>rlbmuajgezfiddjzlyeoupxpqubkpt.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1303</td>\n",
       "      <td>hxjwafskxmlfaotwaklzuwuccsbxfu.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1304</td>\n",
       "      <td>oyodauphffgmzmvqlykcfvmyxartok.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305</td>\n",
       "      <td>uppvtemhkneqggtbfubeccqjvyefiw.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1306</td>\n",
       "      <td>mzjjvzkzvqmyukzsegtoktaslejcdz.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1307</td>\n",
       "      <td>dbjdjyhnbvblwhhnfzaahzhdezbbib.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1308 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   image  target\n",
       "0     tfymlmdkpzkqdjhdxyhnoeuqszxphw.jpg       0\n",
       "1     rxgismgsvmaayzjarbfjaljhqmpbrt.jpg       1\n",
       "2     uuzshfrhkgrkolhwdvliqauzulurnz.jpg       0\n",
       "3     mjspxsagzusaznvnyxgamtrlqkqklp.jpg       0\n",
       "4     rlbmuajgezfiddjzlyeoupxpqubkpt.jpg       1\n",
       "...                                  ...     ...\n",
       "1303  hxjwafskxmlfaotwaklzuwuccsbxfu.jpg       0\n",
       "1304  oyodauphffgmzmvqlykcfvmyxartok.jpg       1\n",
       "1305  uppvtemhkneqggtbfubeccqjvyefiw.png       1\n",
       "1306  mzjjvzkzvqmyukzsegtoktaslejcdz.jpg       0\n",
       "1307  dbjdjyhnbvblwhhnfzaahzhdezbbib.jpg       0\n",
       "\n",
       "[1308 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train_labels.csv\")\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to convert this images into an array, in as much as it can be done in so many ways, i personally love the OpenCV library when it comes to images. In subsequent steps, i use the OpenCV library to resize the image into a 32 by 32 size, and convert to an array based on raw pixel intensity. Since i used the .flatten() method, i am simply flattening the 3 RGB channels such that we have a 32*32*3 = 3072 dimensional feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_array(image, size = (32,32)):\n",
    "    return cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save empty list to store data, and keep data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = train.target.values\n",
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the whole conversion process takes place. After converting per image, i append it to my empty data list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please hold on Boss, it will take some time to convert images. loading.........\n",
      "I'm done with the conversion process Boss\n"
     ]
    }
   ],
   "source": [
    "print(\"Please hold on Boss, it will take some time to convert images. loading.........\")\n",
    "for imagepath in train.image:\n",
    "    image = cv2.imread(\"train_images/\"+imagepath)\n",
    "    features = convert_image_to_array(image)\n",
    "    data.append(features)\n",
    "print(\"I'm done with the conversion process Boss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Small Preprocessing\n",
    "To train a simple neural network using Cross-entropy loss function, i need to convert my labels into a vector format. I also scaled my data too to prepare for training. To understand more about Cross-entropy loss function and other loss functions, Read elisha's blog, i've provided the links in the readme since it was a major reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np_utils.to_categorical(labels,2)\n",
    "data = np.array(data) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test, i used a 20% test set here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset, trainlabels, testlabels = train_test_split(data, labels, test_size = 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see how label looks like now, full vector format\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Building the model\n",
    "This is a siomple feed forward neural network of architecture 3072-768-384-2, This just simply means that my input layer has 3072 nodes, from (32*32*3) which is the size and flattened channels of my input image, Then we have two hidden layers, one with 768 and the other with 384 nodes. These node counts were chosen at my discresion since the aim of this notebook is to build a simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Output layer has 2 nodes, and lastly a sigmoid, activation function as recommended by Elisha's blog. find links in the readme file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used sigmoid because it is a classification task, using it with cross-entropy loss is good since this is a Binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(768, input_dim=3072, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(768, input_dim = 3072, init = \"uniform\", activation = \"relu\"))\n",
    "model.add(Dense(384, activation = \"relu\", kernel_initializer = \"uniform\"))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training the model\n",
    "The next step is to train the model, to do this, I set the learning rate parameter of the SGD optimizer to 0.01, I used binary_crossentropy as the loss function for the network since we are dealing with just 2 labels, I used a batch size of 128 as well as trained for 100 epochs. With all these set, I'm ready to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started training the model boss, i'm running as fast as possible.\n",
      "Epoch 1/100\n",
      "1046/1046 [==============================] - 1s 699us/step - loss: 0.6979 - accuracy: 0.5086\n",
      "Epoch 2/100\n",
      "1046/1046 [==============================] - 0s 442us/step - loss: 0.6843 - accuracy: 0.5521\n",
      "Epoch 3/100\n",
      "1046/1046 [==============================] - 0s 430us/step - loss: 0.6752 - accuracy: 0.5846\n",
      "Epoch 4/100\n",
      "1046/1046 [==============================] - 0s 444us/step - loss: 0.6706 - accuracy: 0.5956\n",
      "Epoch 5/100\n",
      "1046/1046 [==============================] - 0s 429us/step - loss: 0.6590 - accuracy: 0.6052\n",
      "Epoch 6/100\n",
      "1046/1046 [==============================] - 0s 437us/step - loss: 0.6531 - accuracy: 0.6377\n",
      "Epoch 7/100\n",
      "1046/1046 [==============================] - 0s 441us/step - loss: 0.6475 - accuracy: 0.6544\n",
      "Epoch 8/100\n",
      "1046/1046 [==============================] - 0s 437us/step - loss: 0.6405 - accuracy: 0.6549\n",
      "Epoch 9/100\n",
      "1046/1046 [==============================] - 0s 445us/step - loss: 0.6313 - accuracy: 0.6898\n",
      "Epoch 10/100\n",
      "1046/1046 [==============================] - 0s 440us/step - loss: 0.6331 - accuracy: 0.6759\n",
      "Epoch 11/100\n",
      "1046/1046 [==============================] - 0s 446us/step - loss: 0.6303 - accuracy: 0.6745\n",
      "Epoch 12/100\n",
      "1046/1046 [==============================] - 0s 440us/step - loss: 0.6183 - accuracy: 0.6993\n",
      "Epoch 13/100\n",
      "1046/1046 [==============================] - 0s 433us/step - loss: 0.6182 - accuracy: 0.6955\n",
      "Epoch 14/100\n",
      "1046/1046 [==============================] - 0s 437us/step - loss: 0.6095 - accuracy: 0.7055\n",
      "Epoch 15/100\n",
      "1046/1046 [==============================] - 0s 444us/step - loss: 0.6062 - accuracy: 0.6974\n",
      "Epoch 16/100\n",
      "1046/1046 [==============================] - 0s 457us/step - loss: 0.6070 - accuracy: 0.7041\n",
      "Epoch 17/100\n",
      "1046/1046 [==============================] - 0s 459us/step - loss: 0.5980 - accuracy: 0.7070\n",
      "Epoch 18/100\n",
      "1046/1046 [==============================] - 0s 466us/step - loss: 0.5928 - accuracy: 0.7060\n",
      "Epoch 19/100\n",
      "1046/1046 [==============================] - 1s 479us/step - loss: 0.5973 - accuracy: 0.70460s - loss: 0.5941 - accuracy: 0.71\n",
      "Epoch 20/100\n",
      "1046/1046 [==============================] - 0s 467us/step - loss: 0.5811 - accuracy: 0.7228\n",
      "Epoch 21/100\n",
      "1046/1046 [==============================] - 0s 476us/step - loss: 0.5778 - accuracy: 0.7256\n",
      "Epoch 22/100\n",
      "1046/1046 [==============================] - 0s 451us/step - loss: 0.5734 - accuracy: 0.7256\n",
      "Epoch 23/100\n",
      "1046/1046 [==============================] - 0s 452us/step - loss: 0.5736 - accuracy: 0.7294\n",
      "Epoch 24/100\n",
      "1046/1046 [==============================] - 0s 455us/step - loss: 0.5665 - accuracy: 0.7323\n",
      "Epoch 25/100\n",
      "1046/1046 [==============================] - 0s 455us/step - loss: 0.5563 - accuracy: 0.7414\n",
      "Epoch 26/100\n",
      "1046/1046 [==============================] - 0s 444us/step - loss: 0.5637 - accuracy: 0.7390\n",
      "Epoch 27/100\n",
      "1046/1046 [==============================] - 0s 453us/step - loss: 0.5504 - accuracy: 0.7447\n",
      "Epoch 28/100\n",
      "1046/1046 [==============================] - 1s 532us/step - loss: 0.5460 - accuracy: 0.7447\n",
      "Epoch 29/100\n",
      "1046/1046 [==============================] - 1s 511us/step - loss: 0.5454 - accuracy: 0.7567\n",
      "Epoch 30/100\n",
      "1046/1046 [==============================] - 0s 453us/step - loss: 0.5393 - accuracy: 0.7596\n",
      "Epoch 31/100\n",
      "1046/1046 [==============================] - 0s 448us/step - loss: 0.5393 - accuracy: 0.7490\n",
      "Epoch 32/100\n",
      "1046/1046 [==============================] - 0s 448us/step - loss: 0.5371 - accuracy: 0.7438\n",
      "Epoch 33/100\n",
      "1046/1046 [==============================] - 0s 465us/step - loss: 0.5215 - accuracy: 0.7758\n",
      "Epoch 34/100\n",
      "1046/1046 [==============================] - 0s 447us/step - loss: 0.5331 - accuracy: 0.7591\n",
      "Epoch 35/100\n",
      "1046/1046 [==============================] - 0s 449us/step - loss: 0.5179 - accuracy: 0.7701\n",
      "Epoch 36/100\n",
      "1046/1046 [==============================] - 0s 457us/step - loss: 0.5219 - accuracy: 0.7553\n",
      "Epoch 37/100\n",
      "1046/1046 [==============================] - 0s 450us/step - loss: 0.5515 - accuracy: 0.7294\n",
      "Epoch 38/100\n",
      "1046/1046 [==============================] - 0s 455us/step - loss: 0.4998 - accuracy: 0.7887\n",
      "Epoch 39/100\n",
      "1046/1046 [==============================] - 0s 456us/step - loss: 0.5242 - accuracy: 0.7457\n",
      "Epoch 40/100\n",
      "1046/1046 [==============================] - 0s 452us/step - loss: 0.5196 - accuracy: 0.7620\n",
      "Epoch 41/100\n",
      "1046/1046 [==============================] - 0s 445us/step - loss: 0.5239 - accuracy: 0.7591\n",
      "Epoch 42/100\n",
      "1046/1046 [==============================] - 0s 458us/step - loss: 0.5006 - accuracy: 0.7768\n",
      "Epoch 43/100\n",
      "1046/1046 [==============================] - 0s 449us/step - loss: 0.4829 - accuracy: 0.7992\n",
      "Epoch 44/100\n",
      "1046/1046 [==============================] - 0s 450us/step - loss: 0.5182 - accuracy: 0.7490\n",
      "Epoch 45/100\n",
      "1046/1046 [==============================] - 0s 475us/step - loss: 0.5162 - accuracy: 0.7424\n",
      "Epoch 46/100\n",
      "1046/1046 [==============================] - 1s 496us/step - loss: 0.4973 - accuracy: 0.7696\n",
      "Epoch 47/100\n",
      "1046/1046 [==============================] - 0s 473us/step - loss: 0.5186 - accuracy: 0.7557\n",
      "Epoch 48/100\n",
      "1046/1046 [==============================] - 0s 461us/step - loss: 0.5067 - accuracy: 0.7753\n",
      "Epoch 49/100\n",
      "1046/1046 [==============================] - 0s 461us/step - loss: 0.4982 - accuracy: 0.7629\n",
      "Epoch 50/100\n",
      "1046/1046 [==============================] - 0s 453us/step - loss: 0.4676 - accuracy: 0.7997\n",
      "Epoch 51/100\n",
      "1046/1046 [==============================] - 0s 466us/step - loss: 0.4745 - accuracy: 0.7959\n",
      "Epoch 52/100\n",
      "1046/1046 [==============================] - 0s 461us/step - loss: 0.5192 - accuracy: 0.7500\n",
      "Epoch 53/100\n",
      "1046/1046 [==============================] - 0s 461us/step - loss: 0.4863 - accuracy: 0.7806\n",
      "Epoch 54/100\n",
      "1046/1046 [==============================] - 0s 468us/step - loss: 0.4721 - accuracy: 0.7887\n",
      "Epoch 55/100\n",
      "1046/1046 [==============================] - 0s 453us/step - loss: 0.4722 - accuracy: 0.7945\n",
      "Epoch 56/100\n",
      "1046/1046 [==============================] - 0s 460us/step - loss: 0.5712 - accuracy: 0.7261\n",
      "Epoch 57/100\n",
      "1046/1046 [==============================] - 0s 457us/step - loss: 0.4719 - accuracy: 0.7916\n",
      "Epoch 58/100\n",
      "1046/1046 [==============================] - 0s 462us/step - loss: 0.4509 - accuracy: 0.8059\n",
      "Epoch 59/100\n",
      "1046/1046 [==============================] - 0s 450us/step - loss: 0.4973 - accuracy: 0.7610\n",
      "Epoch 60/100\n",
      "1046/1046 [==============================] - 0s 460us/step - loss: 0.4214 - accuracy: 0.8298\n",
      "Epoch 61/100\n",
      "1046/1046 [==============================] - 0s 474us/step - loss: 0.4784 - accuracy: 0.7882\n",
      "Epoch 62/100\n",
      "1046/1046 [==============================] - 1s 502us/step - loss: 0.4614 - accuracy: 0.7825\n",
      "Epoch 63/100\n",
      "1046/1046 [==============================] - 0s 453us/step - loss: 0.4400 - accuracy: 0.8093\n",
      "Epoch 64/100\n",
      "1046/1046 [==============================] - 0s 463us/step - loss: 0.4374 - accuracy: 0.8093\n",
      "Epoch 65/100\n",
      "1046/1046 [==============================] - 0s 476us/step - loss: 0.4704 - accuracy: 0.7906\n",
      "Epoch 66/100\n",
      "1046/1046 [==============================] - 0s 468us/step - loss: 0.4289 - accuracy: 0.8222\n",
      "Epoch 67/100\n",
      "1046/1046 [==============================] - 0s 461us/step - loss: 0.5232 - accuracy: 0.7347\n",
      "Epoch 68/100\n",
      "1046/1046 [==============================] - 0s 474us/step - loss: 0.3952 - accuracy: 0.8547\n",
      "Epoch 69/100\n",
      "1046/1046 [==============================] - 0s 468us/step - loss: 0.4359 - accuracy: 0.8121\n",
      "Epoch 70/100\n",
      "1046/1046 [==============================] - 0s 462us/step - loss: 0.4234 - accuracy: 0.8169\n",
      "Epoch 71/100\n",
      "1046/1046 [==============================] - 0s 461us/step - loss: 0.4694 - accuracy: 0.7839\n",
      "Epoch 72/100\n",
      "1046/1046 [==============================] - 0s 459us/step - loss: 0.4231 - accuracy: 0.8107\n",
      "Epoch 73/100\n",
      "1046/1046 [==============================] - 1s 482us/step - loss: 0.4441 - accuracy: 0.8059\n",
      "Epoch 74/100\n",
      "1046/1046 [==============================] - 0s 466us/step - loss: 0.3731 - accuracy: 0.8709\n",
      "Epoch 75/100\n",
      "1046/1046 [==============================] - 0s 471us/step - loss: 0.4164 - accuracy: 0.8126\n",
      "Epoch 76/100\n",
      "1046/1046 [==============================] - 0s 476us/step - loss: 0.4359 - accuracy: 0.8102\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1046/1046 [==============================] - 0s 462us/step - loss: 0.4721 - accuracy: 0.7925\n",
      "Epoch 78/100\n",
      "1046/1046 [==============================] - 0s 448us/step - loss: 0.4417 - accuracy: 0.8002\n",
      "Epoch 79/100\n",
      "1046/1046 [==============================] - 0s 441us/step - loss: 0.3668 - accuracy: 0.8762\n",
      "Epoch 80/100\n",
      "1046/1046 [==============================] - 0s 443us/step - loss: 0.4158 - accuracy: 0.8207\n",
      "Epoch 81/100\n",
      "1046/1046 [==============================] - 0s 446us/step - loss: 0.3846 - accuracy: 0.8470\n",
      "Epoch 82/100\n",
      "1046/1046 [==============================] - 0s 449us/step - loss: 0.4567 - accuracy: 0.7878\n",
      "Epoch 83/100\n",
      "1046/1046 [==============================] - 0s 450us/step - loss: 0.4200 - accuracy: 0.8270\n",
      "Epoch 84/100\n",
      "1046/1046 [==============================] - 0s 442us/step - loss: 0.3677 - accuracy: 0.8595\n",
      "Epoch 85/100\n",
      "1046/1046 [==============================] - 0s 454us/step - loss: 0.4088 - accuracy: 0.8121\n",
      "Epoch 86/100\n",
      "1046/1046 [==============================] - 0s 465us/step - loss: 0.3568 - accuracy: 0.8743\n",
      "Epoch 87/100\n",
      "1046/1046 [==============================] - 0s 451us/step - loss: 0.3780 - accuracy: 0.8423\n",
      "Epoch 88/100\n",
      "1046/1046 [==============================] - 0s 446us/step - loss: 0.4161 - accuracy: 0.8136\n",
      "Epoch 89/100\n",
      "1046/1046 [==============================] - 0s 459us/step - loss: 0.4088 - accuracy: 0.8193\n",
      "Epoch 90/100\n",
      "1046/1046 [==============================] - 0s 456us/step - loss: 0.3618 - accuracy: 0.8714\n",
      "Epoch 91/100\n",
      "1046/1046 [==============================] - 0s 476us/step - loss: 0.4097 - accuracy: 0.8074\n",
      "Epoch 92/100\n",
      "1046/1046 [==============================] - 0s 463us/step - loss: 0.3678 - accuracy: 0.8480\n",
      "Epoch 93/100\n",
      "1046/1046 [==============================] - 0s 453us/step - loss: 0.3225 - accuracy: 0.8853\n",
      "Epoch 94/100\n",
      "1046/1046 [==============================] - 0s 450us/step - loss: 0.4236 - accuracy: 0.8064\n",
      "Epoch 95/100\n",
      "1046/1046 [==============================] - 0s 450us/step - loss: 0.3474 - accuracy: 0.8633\n",
      "Epoch 96/100\n",
      "1046/1046 [==============================] - 0s 462us/step - loss: 0.3477 - accuracy: 0.8609\n",
      "Epoch 97/100\n",
      "1046/1046 [==============================] - 0s 464us/step - loss: 0.3409 - accuracy: 0.8738\n",
      "Epoch 98/100\n",
      "1046/1046 [==============================] - 0s 463us/step - loss: 0.3762 - accuracy: 0.8370\n",
      "Epoch 99/100\n",
      "1046/1046 [==============================] - 0s 446us/step - loss: 0.3456 - accuracy: 0.8633\n",
      "Epoch 100/100\n",
      "1046/1046 [==============================] - 0s 468us/step - loss: 0.3512 - accuracy: 0.8561\n",
      "Hi boss, i'm done training the model\n"
     ]
    }
   ],
   "source": [
    "print(\"Started training the model boss, i'm running as fast as possible.\")\n",
    "sgd = SGD(lr = 0.01)\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = sgd, metrics = [\"accuracy\"])\n",
    "model.fit(trainset, trainlabels, epochs = 100, batch_size =64, verbose = 1)\n",
    "print(\"Hi boss, i'm done training the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Score Model\n",
    "Finally i check the model performance using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking model performance on test set\n",
      "262/262 [==============================] - 0s 486us/step\n",
      "The loss on the test is 0.6865608396420952, and the accuracy is 65.64885377883911%\n"
     ]
    }
   ],
   "source": [
    "print(\"checking model performance on test set\")\n",
    "(loss, accuracy) = model.evaluate(testset, testlabels, batch_size = 64, verbose =1)\n",
    "print(\"The loss on the test is {}, and the accuracy is {}%\".format(loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The End."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### could we have gotten a higher accuracy score and lower loss?\n",
    "yes of course, we are using a simple neural network for a near-complex task, nevertheless we can\n",
    "\n",
    "1. Increase number of epochs for training\n",
    "2. Use the Adam Optimizer as recommended by Elisha's blog \n",
    "3. Tweak the learning rate to suit it\n",
    "4. Pick better optimized parameters for the hidden neural network layers.\n",
    "5. Check for best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
